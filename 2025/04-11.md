# April 11, 2025 Deep Dive

## Logistics

12-1 PM EDT / 9-10AM PDT / 4-5PM GMT

## Agenda

* Private Proof API (60 minutes)

## Resources

* [Minutes](https://docs.google.com/document/d/1b6vupXweKHMBkJee307NBEgeDTwG43KxKOFsQhWHpoM/edit?usp=sharing)
* [Chair Slides](https://docs.google.com/presentation/d/1m0U3OLrxcNIrpbDWubLEuanDG5GQXyqf6lD2KCMdZ1s/edit?usp=sharing)
* [Private Proof API Deep Dive](https://docs.google.com/presentation/d/1AA0g9GqHqrKIOG_qEl1u8hbRSCBzGWpFnJsE4Nad7w0/edit?usp=sharing)

## Minutes
Sam: 
Sigma Protocol: Prover generates commitments, and the verifier generates a challenge randomly from a challenge set. The prover computes something over the challenge using their secret knowledge, and sends that back to the verifier. The verifier can then observe that the prover successfully solved their challenge.

A sigma protocol can be “Zero knowledge” meaning an observer can not gain information about the secret knowledge, despite the verifier learning some property about that secret knowledge. 

Anonymous Credentials: Digital signatures or message authentication codes (MAC)

Range proofs: Using zero knowledge, we can prove that we know a value x, and that it is less than a known threshold value y.

Rate Limiting: Issuer signs a hidden pseudo-random function seed. The user uses the PRF and a counter to generate a token T with counter value up to RATE_LIMIT, such that there can only be up to RATE_LIMIT (token, proof) pairs per context.

Steven: Are you forced to only have one issuer and verifier, or can you support multiple verifiers in this scheme?
Sam: This is public verifiability where the issuer and verifier are disconnected, however those require pairings and have reduced efficiency (something like 2x-3x worse). Rate limits across multiple verifiers would be separate and you’d need some sort of context or rate limiting service to prevent reuse across multiple verifiers. Security issues with pairings as folks aren’t as confident in them. Some standards in the IETF using pairings, such as the BBS drafts. 

Ari: Web API

What are we trying to achieve? - Reduce friction for likely benign users, not increase challenge friction for novel users.
Overall Flow: Browser does some setup to generate private key and credentials. Issuer verifies the request, decides what value T to sign in the credential. The browser then build rate-limited credentials using the issuer response.

Later, the issuer decides on a threshold T* they want to evaluate the range proof against, and embeds it in javascript on a website.

Proposed API:
Some information has to be published in advanced (public key and epoch length/limit)
Tokens are single-keyed to the issuer site, which must match the frame
Clearing cookies also clears tokens
Javascript API available to sites
Server just learns that there exists a private proof token in the browser for which T < T*

Theo: What if the browser tries to re-generate a previously spent token?
Ari: Part of the proof that is sent back contains a pseudonymous ID, of which there can only be RATE_LIMIT per epoch.

Ari: Security Considerations
Rate limit is necessary per context to prevent sites from using building fingerprints using a collection of proof evaluations
Future Work:
Additional Predicates, different algos, Issuer Fungibility (ring of issuers, which any of which can evaluate a token)

Steven: If you’re already sending a pseudononymous ID, what is the difference between that and sending a cookie?

Ari: The token doesn’t provide the same cross-site tracking vector unless it has been abused and distributed between different contexts. The ID is never reused between invocations.


Maxime: Why is the API based on the HTTP API? Why can’t I get a token as a string?
Sam: Incase your renderer is compromised, we don’t want tokens to be exfiltrated causing credential loss. Ari: Also don’t want 3P scripts to be able to generate proofs and send then to other machines.
Steven: Next steps for the API.
Ari: Trying to release a web spec sometime this quarter. Possible dev trial later this year

Nick Doty: How does this help you reduce friction for new visitors?
Ari: This isn’t going to reduce the burden on new visitors as compared to imitators, but it instead is intended to reduce re-identifiability of prior, benign users by replacing cookies with anonymous proofs.

Theo: We can prove something about the signed information at presentation time. Currently this is a range proof. With privacy pass tokens, all of the knowledge has to come in at issuance time.
